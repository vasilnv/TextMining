{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epqSHtVfXFTx"
      },
      "source": [
        "# Text mining\n",
        "## HW 2 MaxEnt on Reuters\n",
        "\n",
        "We are going to train a Multiclass Maximum Entropy (Softmax Regression) to predict the origin of a document coming from the 20newsgroup dataset.\n",
        "\n",
        "This exercise is similar to 01-LinearRegression. The difference is that you'll have to implement the algorithm yourself.\n",
        "\n",
        "For this puprose we'll use PyTorch, and sklearn. Your job is to fill in the missing code into the cells below.\n",
        "\n",
        "You will find the steps you need to perform in the **Task** section in each cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flB8qoV29Xvg"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm9CTv8U_fBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ab54ad-8d96-4674-d18b-15b3c3daafb5"
      },
      "source": [
        "print('Loading data...')\n",
        "\n",
        "# Passing none as we want to train over all the data.\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      categories=None)\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                      categories=None)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrHtsTtdBcDC"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 20\n",
        "# Lambda\n",
        "REG_PARAM = 0.01\n",
        "# Alpha\n",
        "LEARNING_RATE = 1e-02\n",
        "# Number of features\n",
        "MAX_WORDS = 10000\n",
        "# Priting error information after display_step epochs\n",
        "DISPLAY_STEP = 1\n",
        "NUM_CLASSES = np.max(newsgroups_train.target) + 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utot6Yqu_39N"
      },
      "source": [
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='uint8')[y]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u_iBGCW_uf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea42fe9-eb65-4c87-d46c-981b367c5c85"
      },
      "source": [
        "print(NUM_CLASSES, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "\n",
        "tokenizer = TfidfVectorizer(max_features=MAX_WORDS)\n",
        "\n",
        "x_train = tokenizer.fit_transform(newsgroups_train.data).toarray()\n",
        "x_test = tokenizer.transform(newsgroups_test.data).toarray()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "\n",
        "y_train = to_categorical(newsgroups_train.target, num_classes=NUM_CLASSES)\n",
        "y_test = to_categorical(newsgroups_test.target, num_classes=NUM_CLASSES)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (11314, 10000)\n",
            "x_test shape: (7532, 10000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (11314, 20)\n",
            "y_test shape: (7532, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_train))\n",
        "print(y_train.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7vgQ1qOBiK4",
        "outputId": "a37b6ee1-58bf-4eb9-8c73-da46522726e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBfeJBij73Ec"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TwentyNewsGroupsDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        assert(len(self.x) == len(self.y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TwentyNewsGroupsDataset(x_train, y_train)\n",
        "test_dataset = TwentyNewsGroupsDataset(x_test, y_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deFXqhp1Iaxw"
      },
      "source": [
        "# Model initilization\n",
        "\n",
        "Here comes the most interesting part of the model. You'll have to implement Softmax Regression with SGD. The formulas are presented below for you. You don't have to derive them, you can use them as they are, or you can use PyTorch's gradient function to obtain them.\n",
        "\n",
        "## Softmax regression formulas\n",
        "\n",
        "*Keep in mind that those are the final formulas, the derivation of gradients has been omitted, but in order to derive them you must use the chain and quotient rules.*\n",
        "\n",
        "Here is the basic linear (activation) function:\n",
        "\n",
        "$ z_i = x^T w_i + b_i$\n",
        "\n",
        "This is the softmax (prediction) for class i:\n",
        "\n",
        "$\\hat{y}_i = \\sigma(\\textbf{z})_i = \\frac{\\exp(z_i)}{\\sum_{k=1}^{K}{\\exp(z_k)}}$\n",
        "\n",
        "Derivative of the softmax wrt the activation, here $1(i = j)$ is the identity function, which is $1$ if $i = j$ and $0$ otherwise:\n",
        "\n",
        "${\\frac{\\partial}{\\partial w_j} \\sigma(\\textbf{z})_i = \\sigma(\\textbf{z})_i}(1(i = j) - {\\sigma(\\textbf{z})_j})\\ x$\n",
        "\n",
        "Negative cross-entropy, note that this is a dot product of $y$ and $\\hat{y}$, which are K dimentional vectors (y is K dimentional vector with 1 in correct class and 0 everywhere else, so it can be omitted for other classes).\n",
        "\n",
        "$\\mathcal{L_s} = - \\frac{1}{N}\\sum_{i = 1}^N y_{i} \\log(\\hat{y}_{i}) $\n",
        "\n",
        "Gradient of the loss with respect to the weights (i is the correct class):\n",
        "\n",
        "$ \\frac{\\partial }{\\partial w_i} \\mathcal{L_s} = \\hat{y_i}\\ x $\n",
        "\n",
        "Weights update making a step in the direction opposite to the gradient, since we are minimizing the loss and the gradient is always pointing in the direction of the maximim.\n",
        "Alpha is the learning rate.\n",
        "\n",
        "$ w_i = w_i - \\alpha \\frac{\\partial }{\\partial w_i} \\mathcal{L_s} $\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "$ Acc(y, \\hat{y}) = \\frac{1}{N}\\sum_{i = 1}^N 1(arg\\,max_{j \\in K}\\ \\hat{y}_{i,j} = y_i) $\n",
        "\n",
        "## Dimentions of components\n",
        "$ N $ - number of examples\n",
        "\n",
        "$ M $ - number of features\n",
        "\n",
        "$ K $ - number of classes\n",
        "\n",
        "Features input $ x \\in {\\rm I\\!R}^{N \\times M} $\n",
        "\n",
        "Expected class $ y \\in {\\rm I\\!R}^{N \\times M} $\n",
        "\n",
        "Weight matrix $ W \\in {\\rm I\\!R}^{M  \\times K} $\n",
        "\n",
        "Per class bias $ b \\in {\\rm I\\!R}^{K} $\n",
        "\n",
        "## Tasks\n",
        "1. Implement softmax regression using the formulas above;\n",
        "2. Implement accuracy metric, but use cross entropy for optimization. (In the `evaluation` function)\n",
        "\n",
        "## Tips\n",
        "Checking the PyTorch's documentation, and the lecture \"Introduction to PyTorch\". Also you can use all the built-in to compute the gradients!\n",
        "\n",
        "Also in the loss function you can use the [LogSoftmax](https://pytorch.org/docs/master/nn.html?highlight=log_softmax#torch.nn.LogSoftmax) for numerical stability.\n",
        "\n",
        "Check the [sub](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sub_) function of a Tensor, you will most probably need it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5o1W22c5d7m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, features_size, num_classes):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "\n",
        "    self.w = nn.Parameter(torch.randn(features_size, num_classes, dtype=torch.float64))\n",
        "    self.b = nn.Parameter(torch.randn(num_classes, dtype=torch.float64))\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = torch.matmul(x, self.w) + self.b\n",
        "    y_hat = F.softmax(logits, 1)\n",
        "\n",
        "    return (logits, y_hat)\n",
        "\n",
        "model = LogisticRegression(MAX_WORDS, NUM_CLASSES)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH2w_FdLWAVf"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=REG_PARAM)\n",
        "\n",
        "def update_weights(model, x, y):\n",
        "  y = y.type(torch.float64)\n",
        "  logits, y_hat = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.detach().cpu()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoPTBWijWA2M"
      },
      "source": [
        "def evaluate(model, dataset):\n",
        "  #fill the evaluation function, you can change parameters if you like\n",
        "  model.eval()\n",
        "\n",
        "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for x, y in dataloader:\n",
        "    _, y_pred = model(x)\n",
        "    _, predicted_index = torch.max(y_pred, dim=1)\n",
        "    _, y_true_index = torch.max(y, dim=1)\n",
        "\n",
        "    total += y.size(0)\n",
        "    correct += (predicted_index == y_true_index).sum().item()\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Lz4Jo_IijC"
      },
      "source": [
        "# Model training\n",
        "\n",
        "Train your model with calling the `update_weights` function, and cost computation methods. You don't have to modify this section.\n",
        "\n",
        "## Sanity check\n",
        "\n",
        "Your loss should be similar to:\n",
        "\n",
        "Epoch: 0001 cost=4.237748146  \n",
        "Epoch: 0002 cost=2.006925821  \n",
        "Epoch: 0003 cost=0.838360906  \n",
        "Epoch: 0004 cost=0.526503205  \n",
        "Epoch: 0005 cost=0.406159312  \n",
        "Epoch: 0006 cost=0.338935345  \n",
        "Epoch: 0007 cost=0.288057804  \n",
        "Epoch: 0008 cost=0.245860726  \n",
        "Epoch: 0009 cost=0.208140314  \n",
        "Epoch: 0010 cost=0.170706153  \n",
        "Epoch: 0011 cost=0.141715422  \n",
        "Epoch: 0012 cost=0.117129274  \n",
        "Epoch: 0013 cost=0.094932191  \n",
        "Epoch: 0014 cost=0.075968713  \n",
        "Epoch: 0015 cost=0.060179509  \n",
        "Epoch: 0016 cost=0.049887933  \n",
        "Epoch: 0017 cost=0.039890103  \n",
        "Epoch: 0018 cost=0.033839807  \n",
        "Epoch: 0019 cost=0.027970247  \n",
        "Epoch: 0020 cost=0.024634583  \n",
        "Optimization Finished!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI4eJ-st9Qgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f776ef1-9ec5-4bde-e049-6b6eeff9c993"
      },
      "source": [
        "# Training cycle\n",
        "def train(model, dataset):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(1, MAX_EPOCHS+1):\n",
        "    avg_cost = []\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, drop_last=False)\n",
        "    for x, y in (dataloader):\n",
        "      cost = update_weights(model, x, y)\n",
        "\n",
        "      avg_cost.append(cost)\n",
        "    # Display logs per each DISPLAY_STEP\n",
        "    if (epoch) % DISPLAY_STEP == 0:\n",
        "      print (\"Epoch: {:04d} cost={:.9f}\".format(epoch, np.mean(avg_cost)))\n",
        "train(model, train_dataset)\n",
        "print (\"Optimization Finished!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost=2.077966992\n",
            "Epoch: 0002 cost=0.713722233\n",
            "Epoch: 0003 cost=0.381527853\n",
            "Epoch: 0004 cost=0.239921180\n",
            "Epoch: 0005 cost=0.164923919\n",
            "Epoch: 0006 cost=0.120508174\n",
            "Epoch: 0007 cost=0.091987143\n",
            "Epoch: 0008 cost=0.072579703\n",
            "Epoch: 0009 cost=0.058907330\n",
            "Epoch: 0010 cost=0.048655904\n",
            "Epoch: 0011 cost=0.040869345\n",
            "Epoch: 0012 cost=0.034714190\n",
            "Epoch: 0013 cost=0.029770270\n",
            "Epoch: 0014 cost=0.025771626\n",
            "Epoch: 0015 cost=0.022453961\n",
            "Epoch: 0016 cost=0.019547111\n",
            "Epoch: 0017 cost=0.017104139\n",
            "Epoch: 0018 cost=0.015096708\n",
            "Epoch: 0019 cost=0.013260410\n",
            "Epoch: 0020 cost=0.011861033\n",
            "Optimization Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZPWLMWEEOxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5889e8bc-d700-4268-de8f-80e8d0da5ad8"
      },
      "source": [
        "print(\"Training datset\", evaluate(model, train_dataset))\n",
        "print(\"Test datset\", evaluate(model, test_dataset))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training datset 0.9994696835778681\n",
            "Test datset 0.8182421667551779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8VBu0DGNhM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}